




```{r}
# Chargement des biblioth√®ques n√©cessaires
library(caret)          # Pour la validation crois√©e, le pr√©traitement et l'√©valuation
library(e1071)          # Na√Øve Bayes
library(class)          # KNN
library(randomForest)   # Random Forest
library(nnet)           # R√©gression logistique pour le stacking
library(adabag)         # AdaBoost
library(dplyr)          # Manipulation de donn√©es
library(rpart)          # Arbre de d√©cision
library(rpart.plot)     # Visualisation d'arbre

# Chargement du dataset UFC
ufc_data <- read.csv("ufc_top12_data_synthetic_combined.csv", header = TRUE, sep = ",", dec = ".")

# Conversion de Winner en facteur
ufc_data$Winner <- as.factor(ufc_data$Winner)

# M√©lange des donn√©es et s√©paration Train/Test (85%-15%)
set.seed(123)
ufc_data <- ufc_data[sample(nrow(ufc_data)), ]
trainIndex <- sample(1:nrow(ufc_data), size = round(0.85 * nrow(ufc_data)))  
data_train <- ufc_data[trainIndex, ]
data_test <- ufc_data[-trainIndex, ]

# Pr√©traitement : centrage et r√©duction pour les mod√®les sensibles √† l'√©chelle (ex: KNN)
preProc <- preProcess(data_train[, -ncol(data_train)], method = c("center", "scale"))
data_train_scaled <- predict(preProc, data_train[, -ncol(data_train)])
data_test_scaled  <- predict(preProc, data_test[, -ncol(data_test)])

# Ajout de la variable cible aux donn√©es pr√©trait√©es
data_train_scaled$Winner <- data_train$Winner
data_test_scaled$Winner  <- data_test$Winner

# R√©duction de dimensionnalit√© avec PCA pour diminuer la colin√©arit√©
# Ici, on conserve les composantes qui expliquent 95 % de la variance
preProc_pca <- preProcess(data_train_scaled[, -ncol(data_train_scaled)], method = "pca", thresh = 0.95)
data_train_pca <- predict(preProc_pca, data_train_scaled[, -ncol(data_train_scaled)])
data_test_pca  <- predict(preProc_pca, data_test_scaled[, -ncol(data_test_scaled)])

# Reconstitution des donn√©es avec la variable cible
data_train_pca$Winner <- data_train_scaled$Winner
data_test_pca$Winner  <- data_test_scaled$Winner



```

```{r}
# Entra√Ænement du mod√®le Na√Øve Bayes sur les donn√©es transform√©es
model_nb <- naiveBayes(Winner ~ ., data = data_train_pca)

# Pr√©dictions des probabilit√©s sur le jeu de test
pred_nb_prob <- predict(model_nb, data_test_pca, type = "raw")[, 2]

# Conversion des probabilit√©s en classes binaires avec un seuil de 0.5
pred_nb <- ifelse(pred_nb_prob > 0.5, 1, 0)

# √âvaluation du mod√®le via la matrice de confusion
conf_matrix_nb <- confusionMatrix(as.factor(pred_nb), data_test_pca$Winner)

# Calcul de l'erreur absolue moyenne (MAE)
mae_nb <- mean(abs(pred_nb - as.numeric(as.character(data_test_pca$Winner))))

# Affichage des r√©sultats
print(conf_matrix_nb)
print(paste("Accuracy Na√Øve Bayes avec PCA :", round(conf_matrix_nb$overall["Accuracy"] * 100, 2), "%"))
print(paste("MAE Na√Øve Bayes avec PCA :", round(mae_nb, 4)))



```


```{r}

#  KNN : Optimisation de k sur les donn√©es pr√©trait√©es
k_values <- 1:10
accuracies_knn <- sapply(k_values, function(k) {
  knn_model <- knn(train = data_train_scaled[, -ncol(data_train_scaled)], 
                   test = data_test_scaled[, -ncol(data_test_scaled)], 
                   cl = data_train_scaled$Winner, 
                   k = k, prob = TRUE)
  mean(knn_model == data_test_scaled$Winner)
})
best_k <- k_values[which.max(accuracies_knn)]
print(paste("Meilleur k trouv√© :", best_k))

# Pr√©diction avec le meilleur k et extraction des probabilit√©s pour la classe "1"
knn_model <- knn(train = data_train_scaled[, -ncol(data_train_scaled)], 
                 test = data_test_scaled[, -ncol(data_test_scaled)], 
                 cl = data_train_scaled$Winner, 
                 k = best_k, prob = TRUE)
# Extraction de la probabilit√© pour la classe "1"
pred_knn_prob <- ifelse(knn_model == "1", attr(knn_model, "prob"), 1 - attr(knn_model, "prob"))
pred_knn <- ifelse(pred_knn_prob > 0.5, 1, 0)
accuracy_knn <- mean(pred_knn == as.numeric(as.character(data_test_scaled$Winner)))
mae_knn <- mean(abs(pred_knn - as.numeric(as.character(data_test_scaled$Winner))))

print(paste("Accuracy KNN (k=", best_k, ") :", round(accuracy_knn * 100, 2), "%"))
print(paste("MAE KNN :", round(mae_knn, 4)))



```

```{r}
# Random Forest : suppression de maxnodes et ajustement de mtry
rf_model <- randomForest(Winner ~ ., data = data_train, 
                         ntree = 100, 
                         mtry = 4,    # Ajustement de mtry (par rapport √† 3)
                         importance = TRUE)
# Obtenir les probabilit√©s de la classe "1"
pred_rf_prob <- predict(rf_model, data_test, type = "prob")[,2]
pred_rf <- ifelse(pred_rf_prob > 0.5, 1, 0)
conf_matrix_rf <- confusionMatrix(as.factor(pred_rf), data_test$Winner)
mae_rf <- mean(abs(pred_rf - as.numeric(as.character(data_test$Winner))))

print(paste("Accuracy Random Forest :", round(conf_matrix_rf$overall["Accuracy"] * 100, 2), "%"))
print(paste("MAE Random Forest :", round(mae_rf, 4)))






```
```{r}
# AdaBoost : augmentation de mfinal pour potentiellement am√©liorer la performance
adaboost_model <- boosting(Winner ~ ., data = data_train, 
                           mfinal = 10,   
                           coeflearn = "Breiman")
prediction_adaboost <- predict(adaboost_model, data_test)
# Extraction de la probabilit√© pour la classe "1"
pred_adaboost_prob <- prediction_adaboost$prob[,2]
pred_adaboost <- ifelse(pred_adaboost_prob > 0.5, 1, 0)
conf_matrix_adaboost <- confusionMatrix(as.factor(pred_adaboost), data_test$Winner)
mae_adaboost <- mean(abs(pred_adaboost - as.numeric(as.character(data_test$Winner))))

print(paste("Accuracy AdaBoost :", round(conf_matrix_adaboost$overall["Accuracy"] * 100, 2), "%"))
print(paste("MAE AdaBoost :", round(mae_adaboost, 4)))

```
```{r}
# Cr√©ation du dataset pour le m√©tamod√®le en utilisant les probabilit√©s
stacked_data <- data.frame(
  pred_nb = pred_nb_prob,
  pred_knn = pred_knn_prob,
  pred_rf = pred_rf_prob,
  pred_adaboost = pred_adaboost_prob,
  Winner = as.numeric(as.character(data_test$Winner))
)

# M√©tamod√®le : R√©gression Logistique
meta_model <- glm(Winner ~ ., data = stacked_data, family = binomial)
pred_final_prob <- predict(meta_model, stacked_data, type = "response")
pred_final <- ifelse(pred_final_prob > 0.5, 1, 0)
accuracy_stacking <- mean(pred_final == stacked_data$Winner)
mae_stacking <- mean(abs(pred_final - stacked_data$Winner))

print(paste("Accuracy Stacking :", round(accuracy_stacking * 100, 2), "%"))
print(paste("MAE Stacking :", round(mae_stacking, 4)))


```
```{r}
# üîπ Comparaison des mod√®les
print("Comparaison des mod√®les :")
print(paste("Accuracy Na√Øve Bayes :", round(conf_matrix_nb$overall["Accuracy"] * 100, 2), "%"))
print(paste("MAE Na√Øve Bayes :", round(mae_nb, 4)))

print(paste("Accuracy KNN :", round(accuracy_knn * 100, 2), "%"))
print(paste("MAE KNN :", round(mae_knn, 4)))

print(paste("Accuracy Random Forest :", round(conf_matrix_rf$overall["Accuracy"] * 100, 2), "%"))
print(paste("MAE Random Forest :", round(mae_rf, 4)))

print(paste("Accuracy AdaBoost :", round(conf_matrix_adaboost$overall["Accuracy"] * 100, 2), "%"))
print(paste("MAE AdaBoost :", round(mae_adaboost, 4)))

print(paste("Accuracy Stacking :", round(accuracy_stacking * 100, 2), "%"))
print(paste("MAE Stacking :", round(mae_stacking, 4)))


```
```{r}
#XGBOOST #Gradient Boosting √† venir...




```




