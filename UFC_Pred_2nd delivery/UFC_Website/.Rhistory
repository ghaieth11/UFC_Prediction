# Charger les librairies nécessaires
library(dplyr)
library(reticulate)
library(keras3)
library(tensorflow)
data <- read.csv("ufc_top12_data_synthetic_combined.csv")
# Vérifier les premières lignes du dataset
print(head(data))
data2 <- data %>% select(
BlueAvgSigStrLanded, RedAvgSigStrLanded,
BlueAvgSigStrPct, RedAvgSigStrPct,
BlueAvgTDLanded, RedAvgTDLanded,
BlueReachCms, RedReachCms,
BlueHeightCms, RedHeightCms,
BlueAge, RedAge,RedWins,BlueWins, Winner
)
data2 <- data %>% select(
BlueAvgSigStrLanded, RedAvgSigStrLanded,
BlueAvgSigStrPct, RedAvgSigStrPct,
BlueAvgTDLanded, RedAvgTDLanded,
BlueReachCms, RedReachCms,
BlueHeightCms, RedHeightCms,
BlueAge, RedAge, Winner
)
data2 <- data %>% select(
BlueAvgSigStrLanded, RedAvgSigStrLanded,
BlueAvgSigStrPct, RedAvgSigStrPct,
BlueAvgTDLanded, RedAvgTDLanded,
BlueAvgTDPct, RedAvgTDPct,
BlueAge, RedAge, Winner
)
data2$Winner <- ifelse(data2$Winner == "Red", 0, 1)
#faire scaling pour les données
print(table(data2$Winner))
X <- as.matrix(data2[, -ncol(data2)])  # Toutes les colonnes sauf 'Winner'
Y <- as.matrix(data2$Winner)  # Variable cible
# Diviser les données en train et test
n <- nrow(data2)
train_index <- sample(1:n, size = 0.8 * n)
X_train <- X[train_index, , drop = FALSE]
Y_train <- Y[train_index, , drop = FALSE]
X_test  <- X[-train_index, , drop = FALSE]
Y_test  <- Y[-train_index, , drop = FALSE]
model <- keras_model_sequential()
model %>%
layer_dense(units = 64, activation = "relu", input_shape = c(ncol(X))) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
# Charger les librairies nécessaires
library(dplyr)
library(reticulate)
library(keras3)
library(tensorflow)
data <- read.csv("ufc_top12_data_synthetic_combined.csv")
# Vérifier les premières lignes du dataset
print(head(data))
data2 <- data %>% select(
BlueAvgSigStrLanded, RedAvgSigStrLanded,
BlueAvgSigStrPct, RedAvgSigStrPct,
BlueAvgTDLanded, RedAvgTDLanded,
BlueAvgTDPct, RedAvgTDPct,
BlueAge, RedAge, Winner
)
data2$Winner <- ifelse(data2$Winner == "Red", 0, 1)
#faire scaling pour les données
print(table(data2$Winner))
X <- as.matrix(data2[, -ncol(data2)])  # Toutes les colonnes sauf 'Winner'
Y <- as.matrix(data2$Winner)  # Variable cible
data2 <- data %>% select(
BlueAvgSigStrLanded, RedAvgSigStrLanded,
BlueAvgSigStrPct, RedAvgSigStrPct,
BlueAvgTDLanded, RedAvgTDLanded,
BlueAvgTDPct, RedAvgTDPct,
BlueAge, RedAge, Winner
)
#data2$Winner <- ifelse(data2$Winner == "Red", 0, 1)
#faire scaling pour les données
print(table(data2$Winner))
X <- as.matrix(data2[, -ncol(data2)])  # Toutes les colonnes sauf 'Winner'
Y <- as.matrix(data2$Winner)  # Variable cible
# Diviser les données en train et test
n <- nrow(data2)
train_index <- sample(1:n, size = 0.8 * n)
X_train <- X[train_index, , drop = FALSE]
Y_train <- Y[train_index, , drop = FALSE]
X_test  <- X[-train_index, , drop = FALSE]
Y_test  <- Y[-train_index, , drop = FALSE]
model <- keras_model_sequential()
model %>%
layer_dense(units = 64, activation = "relu", input_shape = c(ncol(X))) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model <- keras_model_sequential()
model %>%
layer_dense(units = 64, activation = "relu", input_shape = c(ncol(X))) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
# Entraîner le modèle
history <- model %>% fit(
X_train, Y_train,
epochs = 100,
batch_size = 16,
validation_split = 0.2,
verbose = 1)
score <- model %>% evaluate(X_test, Y_test, verbose = 0)
cat("Test loss:", score$loss, "\n")
cat("Test accuracy:", score$accuracy, "\n")
predictions <- model %>% predict(X_test)
print(head(predictions))
predictions_binary <- ifelse(predictions > 0.5, 1, 0)
print(head(predictions_binary))
comparison <- data.frame(Predicted = predictions_binary, Actual = Y_test)
comparison
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, activation = "relu", input_shape = c(ncol(X))) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
# Entraîner le modèle
history <- model %>% fit(
X_train, Y_train,
epochs = 100,
batch_size = 16,
validation_split = 0.2,
verbose = 1)
score <- model %>% evaluate(X_test, Y_test, verbose = 0)
cat("Test loss:", score$loss, "\n")
cat("Test accuracy:", score$accuracy, "\n")
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, activation = "relu", input_shape = c(ncol(X))) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
# Entraîner le modèle
history <- model %>% fit(
X_train, Y_train,
epochs = 100,
batch_size = 16,
validation_split = 0.2,
verbose = 1)
score <- model %>% evaluate(X_test, Y_test, verbose = 0)
cat("Test loss:", score$loss, "\n")
cat("Test accuracy:", score$accuracy, "\n")
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, activation = "relu", input_shape = c(ncol(X))) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 32, activation = "sigmoid") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
# Entraîner le modèle
history <- model %>% fit(
X_train, Y_train,
epochs = 100,
batch_size = 16,
validation_split = 0.2,
verbose = 1)
score <- model %>% evaluate(X_test, Y_test, verbose = 0)
cat("Test loss:", score$loss, "\n")
cat("Test accuracy:", score$accuracy, "\n")
model <- keras_model_sequential()
model %>%
layer_dense(units = 64, activation = "relu", input_shape = c(ncol(X))) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
# Entraîner le modèle
history <- model %>% fit(
X_train, Y_train,
epochs = 100,
batch_size = 16,
validation_split = 0.2,
verbose = 1)
score <- model %>% evaluate(X_test, Y_test, verbose = 0)
cat("Test loss:", score$loss, "\n")
cat("Test accuracy:", score$accuracy, "\n")
predictions <- model %>% predict(X_test)
print(head(predictions))
predictions_binary <- ifelse(predictions > 0.5, 1, 0)
print(head(predictions_binary))
comparison <- data.frame(Predicted = predictions_binary, Actual = Y_test)
comparison
model <- keras_model_sequential()
model %>%
layer_dense(units = 10, activation = "relu", input_shape = c(ncol(X))) %>%
layer_dense(units = 4, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
# Entraîner le modèle
history <- model %>% fit(
X_train, Y_train,
epochs = 100,
batch_size = 16,
validation_split = 0.2,
verbose = 1)
score <- model %>% evaluate(X_test, Y_test, verbose = 0)
cat("Test loss:", score$loss, "\n")
cat("Test accuracy:", score$accuracy, "\n")
